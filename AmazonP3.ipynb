{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yLev21E9DWox",
        "outputId": "a4e184f6-9513-4f83-cbbe-fe182f8a78df"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "import numpy as np # linear algebra\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "%matplotlib inline\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split, cross_val_predict, GridSearchCV\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix, roc_curve, roc_auc_score, f1_score, accuracy_score, classification_report\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import roc_curve, auc,roc_auc_score\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn import metrics\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from pprint import pprint\n",
        "import scipy as sp\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from nltk.corpus import stopwords\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import confusion_matrix, roc_auc_score, classification_report, accuracy_score, f1_score\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier  # Example classifier\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "#from imblearn.over_sampling import SMOTE\n",
        "from sklearn.datasets import load_digits\n",
        "from sklearn.feature_selection import SelectKBest, chi2\n",
        "from nltk.tokenize import RegexpTokenizer\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = pd.read_csv('/content/gdrive/MyDrive/CS274/CS274P01PT2/train.csv').fillna(str(0))\n",
        "test_data = pd.read_csv('/content/gdrive/MyDrive/CS274/CS274P01PT2/test.csv').fillna(str(0))"
      ],
      "metadata": {
        "id": "2a1stTniGH8l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for v in train_data['vote']:\n",
        "  if v == None:\n",
        "    v = str(0)\n",
        "  else:\n",
        "    v = str(v)\n",
        "\n",
        "\n",
        "\n",
        "X_train = (train_data['category']+', ' +train_data['category']+', '+train_data['summary']  + ', '+ train_data['reviewText']+', '+str(train_data['vote'])).fillna('')#constructs train and test data\n",
        "X_test = (test_data['category'] + ', '+ test_data['category'] + ', '+ test_data['summary']  + ', ' +''+test_data['reviewText']+', '+str(test_data['vote'])).fillna('')\n",
        "y_train = train_data['overall'].astype(int)\n",
        "y_test = []\n",
        "#print(X_train.shape)"
      ],
      "metadata": {
        "id": "2bq07C0oEzpe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "exc_dict = {             #causes overfit, dont run this block!!!!!\n",
        "    \"1star\": \"One Star\",\n",
        "    \"2star\": \"Two Stars\",\n",
        "    \"3star\": \"Three Stars\",\n",
        "    \"4star\": \"Four Stars\",\n",
        "    \"5star\": \"Five Stars\",\n",
        "    \"negatve\":\"bad|upset|wrong|annoy|disappointed|don't|not|waste|error|junk|terrible\",\n",
        "    \"positive\": \"fast|enjoy|pleased|amazing|fantastic|recoommend|good|happy|great|genuine\",\n",
        "    \"neutral\": \"ok|fine\"\n",
        "}#the dictionary for some keywords extraction\n",
        "\n",
        "extrains = []#extracts keywords from review texts\n",
        "extests = []\n",
        "for x in train_data[\"reviewText\"]:\n",
        "  extraction_train = \"\"\n",
        "\n",
        "  for feature, w in exc_dict.items():\n",
        "    #print(x[\"summary\":])\n",
        "    if w in x:\n",
        "      extraction_train+=feature\n",
        "      extraction_train+=\" \"\n",
        "\n",
        "    #print(extraction_train)\n",
        "  extrains.append(extraction_train)\n",
        "  #X_train = X_train + \", \" + extraction_train\n",
        "for x in test_data[\"reviewText\"]:\n",
        "  extraction_test = \"\"\n",
        "  for feature, w in exc_dict.items():\n",
        "    if w in x:\n",
        "      extraction_test+=feature\n",
        "      extraction_test += \" \"\n",
        "  extests.append(extraction_test)\n",
        "  #X_test = X_test + \", \" + extraction_test\n",
        "i = 0#add the result to train and test data\n",
        "for er in extrains:\n",
        "  X_train[i]+= er\n",
        "  i+=1\n",
        "j = 0\n",
        "for er in extests:\n",
        "  X_test[j]+= er\n",
        "  j+=1\n",
        "\n",
        "extrainss = []#extracts keywords from summary\n",
        "extestss = []\n",
        "for x in train_data[\"summary\"]:\n",
        "  extraction_train = \"\"\n",
        "\n",
        "  for feature, w in exc_dict.items():\n",
        "    #print(x[\"summary\":])\n",
        "    if w in x:\n",
        "      extraction_train+=feature\n",
        "      extraction_train+=\" \"\n",
        "\n",
        "    #print(extraction_train)\n",
        "  extrainss.append(extraction_train)\n",
        "  #X_train = X_train + \", \" + extraction_train\n",
        "for x in test_data[\"summary\"]:\n",
        "  extraction_test = \"\"\n",
        "  for feature, w in exc_dict.items():\n",
        "    if w in x:\n",
        "      extraction_test+=feature\n",
        "      extraction_test += \" \"\n",
        "  extestss.append(extraction_test)\n",
        "  #X_test = X_test + \", \" + extraction_test\n",
        "i = 0#add the result to train and test data\n",
        "for er in extrainss:\n",
        "  X_train[i]+= er\n",
        "  i+=1\n",
        "j = 0\n",
        "for er in extestss:\n",
        "  X_test[j]+= er\n",
        "  j+=1"
      ],
      "metadata": {
        "id": "F22IVCu2VfFP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus.reader.ycoe import wordpunct_tokenize #does not make a huge difference to the result\n",
        "#nltk.download('stopwords'), stopwords has significant negative effect, so we deleted it\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "stopWords = set(stopwords.words('english'))\n",
        "tokenizer = RegexpTokenizer(r'\\w+')\n",
        "puncs = set(['.','$', '/'])\n",
        "ps = PorterStemmer()\n",
        "wnl = WordNetLemmatizer()#use stemmer and Lemmatizer to handel the train and test words, does not have significant effect\n",
        "#tokens_train = wordpunct_tokenize(X_train)\n",
        "for x in X_train:\n",
        "  #print(x)\n",
        "  words = word_tokenize(x)\n",
        "  for word in words:\n",
        "    word = ps.stem(word)\n",
        "    word = wnl.lemmatize(word, pos=\"v\")\n",
        "    for p in puncs:\n",
        "      if p in word:\n",
        "        word.replace(p, '')\n",
        "  #wordsFiltered = [w for w in words if w not in stopWords]\n",
        "  x = words\n",
        "for x in X_test:\n",
        "  words = word_tokenize(x)\n",
        "  for word in words:\n",
        "    word = ps.stem(word)\n",
        "    word = wnl.lemmatize(word, pos=\"v\")\n",
        "    for p in puncs:\n",
        "      if p in word:\n",
        "        word.replace(p, '')\n",
        "  #wordsFiltered = [w for w in words if w not in stopWords]\n",
        "  x = words\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "d-t9557_zgoH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "selector1 = SelectKBest(chi2, k = 3000)#Vectorize the train and test set, and use selector to filtor out the most useful features\n",
        "tfidf_vectorizer = TfidfVectorizer(max_features=50000, ngram_range=(1,3))\n",
        "X_train = tfidf_vectorizer.fit_transform(X_train)\n",
        "X_test = tfidf_vectorizer.transform(X_test)\n",
        "\n",
        "X_train = selector1.fit_transform(X_train, y_train)\n",
        "X_test = selector1.transform(X_test)\n",
        "#y_train = [1 if x>3 else 0 for x in y_train]\n",
        "y_test = []"
      ],
      "metadata": {
        "id": "532_Q2lOccuw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " #split the training datasets for testing\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2)\n",
        "X_test, y_test = X_val, y_val"
      ],
      "metadata": {
        "id": "ur0lkdrn4585"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "oversample = SMOTE()#might not be permitted, do not run this block!!!\n",
        "X_train, y_train = oversample.fit_resample(X_train, y_train)"
      ],
      "metadata": {
        "id": "EN5M67uhGA7P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***MODELS***"
      ],
      "metadata": {
        "id": "_UHlz7s3jeW0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from sklearn import datasets\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "from sklearn.svm import LinearSVC\n",
        "best_clf = LogisticRegression( penalty='l2', random_state=23)\n",
        "#y_pred = LinearSVC(random_state=0).fit(X_train, y_train).predict(X_test)\n",
        "y_pred = best_clf.fit(X_train, y_train).predict(X_test)\n",
        "\n",
        "# Calculate the confusion matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "auc = roc_auc_score(y_test, best_clf.predict_proba(X_test), multi_class='ovr')\n",
        "macro_f1 = f1_score(y_test, y_pred, average='macro')\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(\"Confusion Matrix:\")\n",
        "print(cm)\n",
        "print(f\"ROC AUC: {auc}\")\n",
        "print(f\"Macro F1 Score: {macro_f1}\")\n",
        "print(f\"Accuracy: {accuracy}\\n\")"
      ],
      "metadata": {
        "id": "1D13a9aIGpNj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d51689c7-4e09-41bf-d3d3-ef9dc28b8fd2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Confusion Matrix:\n",
            "[[816 234  55  22  28]\n",
            " [300 624 185  74  32]\n",
            " [ 91 237 583 185  58]\n",
            " [ 38  81 172 679 201]\n",
            " [ 36  37  45 181 844]]\n",
            "ROC AUC: 0.883782960877786\n",
            "Macro F1 Score: 0.6069554305418782\n",
            "Accuracy: 0.6073997944501541\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "best_clf = MultinomialNB(force_alpha=True)\n",
        "#y_pred = LinearSVC(random_state=0).fit(X_train, y_train).predict(X_test)\n",
        "y_pred = best_clf.fit(X_train, y_train).predict(X_test)\n",
        "\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "auc = roc_auc_score(y_test, best_clf.predict_proba(X_test), multi_class='ovr')\n",
        "macro_f1 = f1_score(y_test, y_pred, average='macro')\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(\"Confusion Matrix:\")\n",
        "print(cm)\n",
        "print(f\"ROC AUC: {auc}\")\n",
        "print(f\"Macro F1 Score: {macro_f1}\")\n",
        "print(f\"Accuracy: {accuracy}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5TTtwjsZ1uU1",
        "outputId": "8a82acbc-48e0-4f6a-8970-57ecf31b008b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Confusion Matrix:\n",
            "[[752 324  51  14  14]\n",
            " [231 750 172  48  14]\n",
            " [ 57 313 619 128  37]\n",
            " [ 15 143 219 665 129]\n",
            " [ 19  80  72 230 742]]\n",
            "ROC AUC: 0.880689119414332\n",
            "Macro F1 Score: 0.610075943245367\n",
            "Accuracy: 0.60431654676259\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.datasets import make_classification\n",
        "best_clf = MLPClassifier(random_state=5, max_iter=5)\n",
        "#y_pred = LinearSVC(random_state=0).fit(X_train, y_train).predict(X_test)\n",
        "y_pred = best_clf.fit(X_train, y_train).predict(X_test)\n",
        "\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "auc = roc_auc_score(y_test, best_clf.predict_proba(X_test), multi_class='ovr')\n",
        "macro_f1 = f1_score(y_test, y_pred, average='macro')\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(\"Confusion Matrix:\")\n",
        "print(cm)\n",
        "print(f\"ROC AUC: {auc}\")\n",
        "print(f\"Macro F1 Score: {macro_f1}\")\n",
        "print(f\"Accuracy: {accuracy}\\n\")\n"
      ],
      "metadata": {
        "id": "oWLahh9p2n4m",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1928ddc5-b2f7-4534-e145-c0cef6c12d95"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Confusion Matrix:\n",
            "[[800 258  49  23  25]\n",
            " [250 682 193  72  18]\n",
            " [ 71 256 580 203  44]\n",
            " [ 17  70 151 753 180]\n",
            " [ 13  33  40 223 834]]\n",
            "ROC AUC: 0.8961791015306968\n",
            "Macro F1 Score: 0.626194499020307\n",
            "Accuracy: 0.6250428228845495\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (5) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.datasets import make_classification\n",
        "final_classifier =  MLPClassifier(random_state=5, max_iter=4)\n",
        "final_classifier.fit(X_train, y_train)\n",
        "y_test_pred = final_classifier.predict(X_test)\n",
        "test0 = pd.read_csv('/content/gdrive/MyDrive/CS274/CS274P01PT2/test.csv')\n",
        "test_id = test0['id']\n",
        "submission=pd.DataFrame()\n",
        "submission['pred'] = y_test_pred\n",
        "submission['id'] = test_id\n",
        "\n",
        "print(submission.head)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kHyVLHiyNwrd",
        "outputId": "4b9aceff-2ed8-4c33-dbdd-26a758575f74"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<bound method NDFrame.head of       pred     id\n",
            "0        1     a0\n",
            "1        1     a1\n",
            "2        1     a2\n",
            "3        1     a3\n",
            "4        2     a4\n",
            "...    ...    ...\n",
            "4495     5  a4495\n",
            "4496     5  a4496\n",
            "4497     5  a4497\n",
            "4498     5  a4498\n",
            "4499     5  a4499\n",
            "\n",
            "[4500 rows x 2 columns]>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (4) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "submission.to_csv('/content/gdrive/MyDrive/CS274/CS274P01PT3/sub_1.csv', index=False)"
      ],
      "metadata": {
        "id": "Ohkhftt2US2G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "AYG_bqCOpk_s"
      }
    }
  ]
}